{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Los arboles de decisión de clasificación y regresión (CART)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://image.slidesharecdn.com/decisiontree-151015165353-lva1-app6892/95/classification-using-decision-tree-12-638.jpg?cb=1444928106\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La creación de un arbol de decisión binario consiste en dividir recursivamente los datos de entrada usando una función de costo (Greedy). Se tienen dos tipos:\n",
    "\n",
    "* Regresión: La función de costo a minimizar para partir los puntos es la suma de errores al cuadrado.\n",
    "* Clasificación: Se usa la función de costo Gini que provee una indicación de la pureza de los nodos.\n",
    "\n",
    "La pureza se refiere a que tan mezclada estan las muestras de datos asignados a cada nodo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### El indice de Gini\n",
    "\n",
    "Cada partición involucra un atributo y un valor por dicho atributo. Se usa para dividir el dataset de entrada en 2 grupos. \n",
    "\n",
    "Una separación perfecta de los resultados nos brinda un score Gini de 0 mientrasque el peor caso (50/50) de clases en cada grupo resulta en un score de 0.5 para un problema de clasificación binaria."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creación de una partición\n",
    "\n",
    "El particionado implica separar el dataset en 2 listas de filas, dado el indice del atributo y el valor de particionado para dicho atributo. Una vez que tenemos los 2 grupos, se usa el Score de Gini para evaluar el costo del particionado.\n",
    "\n",
    "Tomar en cuenta, que ello involucra iterar sobre cada fila, verificando si el valor del atributo esta debajo o supera el valor de particionado y asignar dicha fila hacia la izquierda o la derecha respectivamente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez que encontramos la mejor partición, se podrá usar como nodo en el arbol de decisión."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### y asi seguimos y seguimos y...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://hellonwheelslifeonehanded.files.wordpress.com/2017/06/when-does-it-end-1.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Terminación\n",
    "\n",
    "Normalmente, se define la terminación usando la profundidad del árbol y/o el número de filas del dataset por nodo.\n",
    "\n",
    "* Profundidad máxima: Es el máximo número de nodos desde la raíz del árbol. Una vez llegado al límite, se detiene el particionado (deja de agregar nodos). Un árbol mas profundo puede producir sobre entrenamiento.\n",
    "* Mínimo número de filas por nodo: Una vez llegado a un cierto número de datos en un nodo, se detiene el particionado. Los nodos con una pequeña cantidad de datos pueden producir sobre-entrenamiento.\n",
    "\n",
    "Obviamente, si en un nodo, todos los datos de entrada fueran de una misma clase, se debe detener el algoritmo de particionado.\n",
    "\n",
    "Cuando el particionado se detiene, el nodo resultante tiene por nombre nodo terminal. Con estos nodos es que se producen las predicciones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Particionado Recursivo\n",
    "\n",
    "Para crear el arbol de decisión, deberemos realizar particionado muchas veces en los grupos generados por nodo.\n",
    "\n",
    "Cada nodo partira el dataset en 2, generando 2 nodos hijos y estos partirán los datos del grupo en 2 y asi sucesivamente hasta cumplir con la condición de terminación y resultando en las hojas o nodos terminales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Links\n",
    "\n",
    "[Implementation of Decision Tree](https://machinelearningmastery.com/implement-decision-tree-algorithm-scratch-python/)<br/>\n",
    "[Decision Trees](http://www.saedsayad.com/decision_tree.htm)<br/>\n",
    "[Spotify example](https://www.youtube.com/watch?v=XDbj6PxaSf0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "            splitter='best')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import tree\n",
    "tree.DecisionTreeClassifier()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
